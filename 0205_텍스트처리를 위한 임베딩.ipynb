{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMsApFVSA9ty2HTKTtQoUFG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y2BGNOoGPjb6","executionInfo":{"status":"ok","timestamp":1675581505284,"user_tz":-540,"elapsed":30073,"user":{"displayName":"최애림","userId":"12177073931316150776"}},"outputId":"717dd154-55b7-4a7c-811c-47a18144f7b4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/study_DL/Study_DL"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RUjtuT5PrwC","executionInfo":{"status":"ok","timestamp":1675581505285,"user_tz":-540,"elapsed":13,"user":{"displayName":"최애림","userId":"12177073931316150776"}},"outputId":"d3bc2a00-91f7-45c4-c206-ae60f3d66b21"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/study_DL/Study_DL\n"]}]},{"cell_type":"markdown","source":["# 텍스트 처리를 위한 임베딩 층을 활용한 간단한 모델 만들기"],"metadata":{"id":"6gO74aSDPf20"}},{"cell_type":"code","source":["from keras.layers import Embedding"],"metadata":{"id":"DyXg47UTQVzN","executionInfo":{"status":"ok","timestamp":1675581507673,"user_tz":-540,"elapsed":2396,"user":{"displayName":"최애림","userId":"12177073931316150776"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 임베딩 layer 쌓기\n","embedding_layer = Embedding(1000,64)   #(samples, sequence_length) -> (samples, sequence_length, embedding_dimensionality)반환 "],"metadata":{"id":"zeXyeCn_QVuU","executionInfo":{"status":"ok","timestamp":1675581507715,"user_tz":-540,"elapsed":49,"user":{"displayName":"최애림","userId":"12177073931316150776"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### 임베딩 층에 사용할 IMDB 데이터 로드"],"metadata":{"id":"cMyUbyafYcY3"}},{"cell_type":"code","source":["from keras.datasets import imdb\n","from tensorflow.keras import preprocessing"],"metadata":{"id":"OTCIZkVSQVpp","executionInfo":{"status":"ok","timestamp":1675581508131,"user_tz":-540,"elapsed":464,"user":{"displayName":"최애림","userId":"12177073931316150776"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)  # num_words: 피처로 사용할 단어 개수\n","\n","train_data.shape, test_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-cSH6NPYnP-","executionInfo":{"status":"ok","timestamp":1675581512672,"user_tz":-540,"elapsed":4604,"user":{"displayName":"최애림","userId":"12177073931316150776"}},"outputId":"f25bf84d-51ba-464b-d0a2-93287f0fb0a1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n"]},{"output_type":"execute_result","data":{"text/plain":["((25000,), (25000,))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["print(train_data[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2190SM3Zxlr","executionInfo":{"status":"ok","timestamp":1675581512673,"user_tz":-540,"elapsed":138,"user":{"displayName":"최애림","userId":"12177073931316150776"}},"outputId":"df61dbb8-66a4-4e74-e45b-1732691c808d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"]}]},{"cell_type":"code","source":["# 리스트를 2D 정수 텐서로 변환\n","x_train = preprocessing.sequence.pad_sequences(train_data, maxlen = 20)  # maxlen: 사용할 텍스트의 길이(가장 빈번한 max_features개의 단어만 사용)\n","x_test = preprocessing.sequence.pad_sequences(test_data, maxlen = 20)\n","\n","x_train.shape, x_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idhTTDwoZPPo","executionInfo":{"status":"ok","timestamp":1675581512675,"user_tz":-540,"elapsed":100,"user":{"displayName":"최애림","userId":"12177073931316150776"}},"outputId":"3c20e130-3020-4748-f553-733b1201feff"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((25000, 20), (25000, 20))"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["x_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zGr_4SipQVmJ","executionInfo":{"status":"ok","timestamp":1675581512676,"user_tz":-540,"elapsed":91,"user":{"displayName":"최애림","userId":"12177073931316150776"}},"outputId":"f6cefe50-41df-4c50-ec51-f290f16ca364"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n","        113,  103,   32,   15,   16, 5345,   19,  178,   32], dtype=int32)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### 모델 생성"],"metadata":{"id":"29H4e5LIYimQ"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Embedding"],"metadata":{"id":"ofhm_p5IQVdD","executionInfo":{"status":"ok","timestamp":1675581512677,"user_tz":-540,"elapsed":53,"user":{"displayName":"최애림","userId":"12177073931316150776"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","\n","# 임베딩\n","model.add(Embedding(10000, 8, input_length = 20))  \n","            # 나중에 임베딩된 입력을 flatten 층에서 펼치기 위해 input_length를 지정함\n","            # 출력 크기는 (10000, 20, 8)이 됨\n","\n","# 임베딩된 단어 펼치기: 3D 텐서가 (10000,20*8)의 2D 텐서가 됨\n","model.add(Flatten())\n","\n","model.add(Dense(1, activation = 'sigmoid'))\n"],"metadata":{"id":"YRxM7nhkQVXz","executionInfo":{"status":"ok","timestamp":1675581515789,"user_tz":-540,"elapsed":2573,"user":{"displayName":"최애림","userId":"12177073931316150776"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### 학습"],"metadata":{"id":"lycKlDpYb5VP"}},{"cell_type":"code","source":["model.compile(optimizer = 'rmsprop',\n","              loss = 'binary_crossentropy',\n","              metrics = ['acc'])\n","\n","history = model.fit(x_train, train_labels,\n","                    batch_size = 32,\n","                    epochs = 10,\n","                    validation_split = 0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzTuHldyQVTw","executionInfo":{"status":"ok","timestamp":1675581557452,"user_tz":-540,"elapsed":41681,"user":{"displayName":"최애림","userId":"12177073931316150776"}},"outputId":"62e0f3aa-e871-44cf-eef1-7ffac69496ea"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - 6s 5ms/step - loss: 0.6690 - acc: 0.6270 - val_loss: 0.6206 - val_acc: 0.6956\n","Epoch 2/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.5426 - acc: 0.7531 - val_loss: 0.5282 - val_acc: 0.7294\n","Epoch 3/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.4608 - acc: 0.7903 - val_loss: 0.5028 - val_acc: 0.7458\n","Epoch 4/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.4191 - acc: 0.8127 - val_loss: 0.4954 - val_acc: 0.7550\n","Epoch 5/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3893 - acc: 0.8272 - val_loss: 0.4958 - val_acc: 0.7536\n","Epoch 6/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3649 - acc: 0.8424 - val_loss: 0.4992 - val_acc: 0.7536\n","Epoch 7/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3432 - acc: 0.8535 - val_loss: 0.5045 - val_acc: 0.7532\n","Epoch 8/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3233 - acc: 0.8644 - val_loss: 0.5116 - val_acc: 0.7512\n","Epoch 9/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.3048 - acc: 0.8728 - val_loss: 0.5210 - val_acc: 0.7468\n","Epoch 10/10\n","625/625 [==============================] - 2s 3ms/step - loss: 0.2878 - acc: 0.8824 - val_loss: 0.5287 - val_acc: 0.7484\n"]}]},{"cell_type":"markdown","source":["- 각 임베딩 시퀀스를 펼치고 하나의 dense층으로 훈련했으므로, 입력 시퀀스에 있는 각 단어를 독립적으로 다룸\n","- 단어 사이의 관계/문장 구조 고려하지 않음\n","  - 예를 들어, 'this movie is a bomb'과 'this movie is the bomb'을 부정적 리뷰로 동일하게 다룰 것\n","- 따라서 각 시퀀스 전체를 고려한 특성을 학습하도록 임베딩 층 위에, 순환 층이나 1D 합성곱층을 추가하는 것이 좋음"],"metadata":{"id":"BzuB8n_PeMdx"}},{"cell_type":"markdown","source":["## 사전 훈련된 임베딩 층을 사용하여 모델 만들기\n","\n","#### Word2Vec\n","- 성별처럼 구체적인 의미가 있는 속성을 잡아냄\n","\n","#### GloVe\n","- 위키디피아 데이터와 커먼 크롤 데이터에서 가져온 수백만 개의 영어 토큰에 대해서 임베딩을 미리 계산해 둠"],"metadata":{"id":"Il6jiVLqh9pI"}},{"cell_type":"markdown","source":["### 원본 IMDB 텍스트 다운 받고 데이터 쌓기"],"metadata":{"id":"SCWe8OADiTsg"}},{"cell_type":"code","source":["import os"],"metadata":{"id":"eWeVhteRh9Ws","executionInfo":{"status":"ok","timestamp":1675583357786,"user_tz":-540,"elapsed":418,"user":{"displayName":"최애림","userId":"12177073931316150776"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["base_dir = '../필요데이터/aclImdb'\n","\n","train_dir = os.path.join(,'train')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","  dir_name = os.path.join(train_dir, label_type)\n","\n","  for fname in sorted(os.listdir(dir_name)):\n","    if fname[-4:] == '.txt':\n","      f = open(os.path.join(dir_name, fname), encoding = 'utf-8')\n","      texts.append(f.read())\n","      f.close()\n","      if label_type == 'neg':\n","        labels.append(0)\n","      else:\n","        labels.append(1)"],"metadata":{"id":"YN3_O-v2QVO3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 토큰화"],"metadata":{"id":"wmNvaRIXk8Ow"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np"],"metadata":{"id":"niraMzAZQVKl","executionInfo":{"status":"ok","timestamp":1675583875167,"user_tz":-540,"elapsed":460,"user":{"displayName":"최애림","userId":"12177073931316150776"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["maxlen = 100\n","training_samples = 200\n","validation_samples = 10000\n","max_words = 10000\n","\n","# 토큰화\n","tokenizer = Tokenizer(num_words = max_words)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequence(texts)\n","print(sequences.shape)"],"metadata":{"id":"GKI_5FKWQVFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_index = tokenizer.word_index\n","print(word_index.shape)\n","print(f'{len(word_index)}개의 고유한 토큰을 찾았습니다.')"],"metadata":{"id":"h4KvsWy1QVAf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pad_sequences(sequences, maxlen = maxlen)\n","\n","labels = np.asarray(labels)\n","print('데이터 텐서의 크기:', data.shape)\n","print('데이터 텐서의 크기:', labels.shape)"],"metadata":{"id":"nH-C2-SFnvBc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","\n","data = data[indices]\n","labels = labels[indices]\n","\n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","\n","x_val = data[training_samples: training_samples + validation_samples]\n","y_val = data[training_samples: training_samples + validation_samples]"],"metadata":{"id":"oAFwdNbEoBR2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 임베딩 전처리"],"metadata":{"id":"-FRymTIgnrLr"}},{"cell_type":"code","source":["# 위키피디아를 사용하여 사전에 계산된 임베딩 내려받고, 파일ㅇ르 파싱하여 단어와 이에 상응하는 벡터 표현을 매핑하는 인덱스 만들기\n","glove_dir = '../필요데이터/'\n","\n","embedding_index = {}\n","\n","f = open(os.path.join(glove_dir, 'glove.6B.100d.txt', encoding = 'utf8'))\n","for line in f:\n","  values = line.split()\n","  word = values[0]\n","  coefs = np.asarray(values[1:], dtype = 'float32')\n","  embedding_index[word] = coefs\n","f.close()\n","\n","print(f'{len(embedding_index)}개의 단어 벡터를 찾았습니다.')"],"metadata":{"id":"2zyKo12xnuU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embedding 층에 주입할 수 있도록 임베딩 행렬 만들기\n","# (max_words, embedding_dim) 크기어야 함\n","\n","embedding_dim = 100\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","for word, i in word_index.items():\n","  embedding_vector = embedding_index.get(word)\n","  if embedding_vector is not None:\n","    embedding_matrix[i] = embedding_vector   # 임베딩 인덱스에 없는 단어는 모두 0이 됨"],"metadata":{"id":"n7kmcIYsnuPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LNpTsHDtnuKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"V0s2CX2knuEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PvDzgT9ent_j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TBIwQrPHnt6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qRpgAdlwnt2E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3muteEzsPPco"},"outputs":[],"source":[]}]}